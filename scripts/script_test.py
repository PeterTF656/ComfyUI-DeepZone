from comfy_script.runtime import *
load()
from comfy_script.runtime.nodes import *

with Workflow():
    image, _ = LoadImage('Screenshot 2024-08-10 174607.png')
    image, _, _ = ImageResize(image, 1024, 1024, 'nearest', 'stretch', 'always', 0)
    image2 = ImageRemoveColor(image, 0, 183, 20, 0, 0, 0, 50)
    PreviewImage(image2)
    florence2_model = DownloadAndLoadFlorence2Model('microsoft/Florence-2-base', 'fp16', 'sdpa', None)
    image3, _, data, _ = Florence2Run(image2, florence2_model, 'grey skin male', 'caption_to_phrase_grounding', True, False, 1024, 3, True, '', 1112031733769435)
    PreviewImage(image3)
    rembg_session = RemBGSession('isnet-anime: anime illustrations', 'CPU')
    # _, _ = ImageRemoveBackground(rembg_session, image)
    florence2_model2 = DownloadAndLoadFlorence2Model('microsoft/Florence-2-base', 'fp16', 'sdpa', None)
    image4, _, data2, _ = Florence2Run(image2, florence2_model2, 'pink yellow skin', 'caption_to_phrase_grounding', True, False, 1024, 3, True, '', 527837226311833)
    PreviewImage(image4)
    image5, _ = CRColorPanel(1024, 1024, 'black', '#000000')
    image6, _ = CRColorPanel(1024, 1024, 'blue', '#000000')
    sam2_model = DownloadAndLoadSAM2Model('sam2_hiera_large.safetensors', 'single_image', 'cuda', 'fp16')
    _, bboxes = Florence2toCoordinates(data, '', False)
    mask = Sam2Segmentation(sam2_model, image2, True, '', '', bboxes, True, None)
    mask = GrowMask(mask, 5, True)
    mask = InvertMask(mask)
    image7 = ImageOverlay(image5, image6, 'None', 'nearest-exact', 1, 1024, 1024, 0, 0, 0, 0, mask)
    PreviewImage(image7)
    model, clip, vae = CheckpointLoaderSimple('autismmixSDXL_autismmixConfetti.safetensors')
    image8, _ = CRColorPanel(1024, 1024, 'red', '#000000')
    sam2_model2 = DownloadAndLoadSAM2Model('sam2_hiera_large.safetensors', 'single_image', 'cuda', 'fp16')
    _, bboxes2 = Florence2toCoordinates(data2, '', False)
    mask2 = Sam2Segmentation(sam2_model2, image2, True, '', '', bboxes2, True, None)
    mask2 = GrowMask(mask2, 5, True)
    mask2 = InvertMask(mask2)
    image9 = ImageOverlay(image7, image8, 'None', 'nearest-exact', 1, 1024, 1024, 0, 0, 0, 0, mask2)
    conditioning, mask3 = RegionalConditioningColorMaskInspire(clip, image9, '#FF0000', 1, 'default', '1girl, (long red hair:1.1),', 0)
    image10 = MaskToImage(mask3)
    PreviewImage(image10)
    # _ = VAEEncode(image, vae)
    PreviewImage(image9)
    image11 = DepthAnythingV2Preprocessor(image2, 'depth_anything_v2_vitl.pth', 1024)
    PreviewImage(image11)
    model, ipadapter = IPAdapterUnifiedLoaderFaceID(model, 'FACEID PLUS V2', 0.6, 'CUDA', None)
    image12, _ = LoadImage('pasted/image (3).png')
    mask4, _ = LoadImage('clipspace/clipspace-mask-3737537.199999988.png [input]')
    clip_vision = CLIPVisionLoader('CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors')
    insightface = IPAdapterInsightFaceLoader('CPU', 'buffalo_l')
    model, _ = IPAdapterFaceID(model, ipadapter, image12, 1, 1, 'linear', 'concat', 0, 1, 'V only', None, mask4, clip_vision, insightface)
    conditioning2 = CLIPTextEncode('score_9, score_8_up, (1girl with long hair:1.05) ,masterpiece, best quality, sexy, aroused, (NSFW:1.15), missionarypose,insertion, uncensored, extremely detailed', clip)
    conditioning3, _ = RegionalConditioningColorMaskInspire(clip, image9, '#0000FF', 1, 'default', '1man, asian man,( naked:1.05),male, handsome man,(blue short hair:1.05), dark skin', 0)
    conditioning4 = ConditioningCombine(conditioning3, conditioning)
    conditioning5 = ConditioningCombine(conditioning2, conditioning4)
    conditioning6 = CLIPTextEncode('text, watermark, blurry, (worst quality:1.4, low quality:1.4, normal quality:1.4), lowres, monochrome, zombie, (interlocked fingers:1.2), earrings, see-through', clip)
    control_net = ControlNetLoader('sdxl_union.safetensors')
    control_net = SetUnionControlNetType(control_net, 'canny/lineart/anime_lineart/mlsd')
    image13 = LineArtPreprocessor(image, 'disable', 1024)
    control_net2 = ControlNetLoader('sdxl_union.safetensors')
    control_net2 = SetUnionControlNetType(control_net2, 'depth')
    positive2, negative2 = ControlNetApplyAdvanced(positive, negative, control_net2, image11, 0.5, 0, 1)
    latent = EmptyLatentImage(1024, 1024, 1)
    latent = KSampler(model, 6, 20, 5, 'euler_ancestral', 'karras', positive2, negative2, latent, 1)
    image14 = VAEDecode(latent, vae)
    PreviewImage(image14)

